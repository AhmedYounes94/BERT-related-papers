# BERT-related Papers

## Downstream task

## Generation
-	[BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model](https://arxiv.org/abs/1902.04094) (NAACL2019 WS)
-	[Pretraining-Based Natural Language Generation for Text Summarization](https://arxiv.org/abs/1902.09243)
-	[MASS: Masked Sequence to Sequence Pre-training for Language Generation](https://arxiv.org/abs/1905.02450) (ICML2019)
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/abs/1905.03197)
    
## Modification (multi-task, masking strategy, etc.)

## Probe

## Multi-lingual

## Inside BERT

## Domain specific

## Multi-modal

## Misc.
